{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afee0a32-8235-4f35-8527-0a813c1e70fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "hostname: e1aa0d4ec9d0\n",
      "cwd: /home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "import platform, socket, os\n",
    "\n",
    "print(\"platform:\", platform.platform())\n",
    "print(\"hostname:\", socket.gethostname())\n",
    "print(\"cwd:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e75015c-3e93-4216-a2cf-0c6fb38f40a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created SparkSession ✅ 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "#проверяем есть ли spark в образе jupyter. Если есть - запускаем, если нет - создаем локально\n",
    "try:\n",
    "    spark\n",
    "    print(\"Using existing SparkSession ✅\", spark.version)\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"features-from-hdfs\").getOrCreate() #Почему local[*]? Это самый стабильный старт в ноутбуке.\n",
    "    print(\"Created SparkSession ✅\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e17124-1351-4633-a0e7-ce71fdcc64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#важное: указываем правильный HDFS\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.defaultFS\", \"hdfs://namenode:8020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30dca960-b656-4cdd-b882-01d4e67a9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#путь до файла с сырыми данными\n",
    "raw_path = \"hdfs://namenode:8020/raw/vacancies/vacancies_raw.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1ec6f7-1d7c-43af-9d63-cdddd52eeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiLine\", True).json(raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ac0aa4-6eb0-42bf-a62e-7645b92f2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------+---------------------+------------------+----------+\n",
      "|published_at|region|salary|skills               |title             |vacancy_id|\n",
      "+------------+------+------+---------------------+------------------+----------+\n",
      "|2026-01-31  |Москва|250000|[python, ml, spark]  |Python ML Engineer|1         |\n",
      "|2026-01-31  |Москва|200000|[sql, spark, airflow]|Data Scientist    |2         |\n",
      "+------------+------+------+---------------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9a2d022-b908-4f25-b103-a19761505a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "exploded = (\n",
    "    df\n",
    "    .withColumn(\"published_date\", F.to_date(\"published_at\")) #нормализую дату для агрегаций\n",
    "    .withColumn(\"skill\", F.explode(\"skills\")) #денормализация массива\n",
    ")\n",
    "\n",
    "exploded.createOrReplaceTempView(\"vacancies_exploded\") #создаю sql-таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "445b88d4-df18-4f58-8b3a-71004161e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#агрегация в sql (делаем sql-витрину, где по дате и региону имеем: 1) сколько вакансий с этим skill, 2) средняя зп по этому skill)\n",
    "features = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  published_date,\n",
    "  region,\n",
    "  skill,\n",
    "  COUNT(*) AS vacancies_cnt,\n",
    "  AVG(salary) AS avg_salary\n",
    "FROM vacancies_exploded\n",
    "GROUP BY published_date, region, skill\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b61c00a5-af40-42d4-8e69-e141b7277b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+-------+-------------+----------+\n",
      "|published_date|region|skill  |vacancies_cnt|avg_salary|\n",
      "+--------------+------+-------+-------------+----------+\n",
      "|2026-01-31    |Москва|ml     |1            |250000.0  |\n",
      "|2026-01-31    |Москва|spark  |2            |225000.0  |\n",
      "|2026-01-31    |Москва|airflow|1            |200000.0  |\n",
      "|2026-01-31    |Москва|python |1            |250000.0  |\n",
      "|2026-01-31    |Москва|sql    |1            |200000.0  |\n",
      "+--------------+------+-------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.show(truncate=False) #truncate выравнивает ячейки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "660f1f5a-10e8-4bd6-894d-147cee82f88e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://namenode:8020/features/vacancies_daily_skill'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = \"hdfs://namenode:8020/features/vacancies_daily_skill\"\n",
    "features.write.mode(\"overwrite\").parquet(out_path) #сохраняю витрину в hdfs\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b50d85f-c4ec-4e96-937c-da54dde8c62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------+-------+-------------+----------+\n",
      "|published_date|region|skill  |vacancies_cnt|avg_salary|\n",
      "+--------------+------+-------+-------------+----------+\n",
      "|2026-01-31    |Москва|ml     |1            |250000.0  |\n",
      "|2026-01-31    |Москва|spark  |2            |225000.0  |\n",
      "|2026-01-31    |Москва|airflow|1            |200000.0  |\n",
      "|2026-01-31    |Москва|python |1            |250000.0  |\n",
      "|2026-01-31    |Москва|sql    |1            |200000.0  |\n",
      "+--------------+------+-------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check = spark.read.parquet(out_path) #чтение parquet (контрольная проверка)\n",
    "check.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2823d17d-c277-4291-88e7-2733ae2062d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
