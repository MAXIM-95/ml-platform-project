{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afee0a32-8235-4f35-8527-0a813c1e70fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platform: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "hostname: e1aa0d4ec9d0\n",
      "cwd: /home/jovyan/work\n"
     ]
    }
   ],
   "source": [
    "import platform, socket, os\n",
    "\n",
    "print(\"platform:\", platform.platform())\n",
    "print(\"hostname:\", socket.gethostname())\n",
    "print(\"cwd:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e75015c-3e93-4216-a2cf-0c6fb38f40a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created SparkSession ✅ 3.5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "#проверяем есть ли spark в образе jupyter. Если есть - запускаем, если нет - создаем локально\n",
    "try:\n",
    "    spark\n",
    "    print(\"Using existing SparkSession ✅\", spark.version)\n",
    "except NameError:\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(\"features-from-hdfs\").getOrCreate() #Почему local[*]? Это самый стабильный старт в ноутбуке.\n",
    "    print(\"Created SparkSession ✅\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e17124-1351-4633-a0e7-ce71fdcc64f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#важное: указываем правильный HDFS\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.defaultFS\", \"hdfs://namenode:8020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30dca960-b656-4cdd-b882-01d4e67a9650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#путь до файла с сырыми данными\n",
    "raw_path = \"hdfs://namenode:8020/raw/vacancies/vacancies_raw.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1ec6f7-1d7c-43af-9d63-cdddd52eeb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"multiLine\", True).json(raw_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ac0aa4-6eb0-42bf-a62e-7645b92f2ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------+---------------------+------------------+----------+\n",
      "|published_at|region|salary|skills               |title             |vacancy_id|\n",
      "+------------+------+------+---------------------+------------------+----------+\n",
      "|2026-01-31  |Москва|250000|[python, ml, spark]  |Python ML Engineer|1         |\n",
      "|2026-01-31  |Москва|200000|[sql, spark, airflow]|Data Scientist    |2         |\n",
      "+------------+------+------+---------------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2d022-b908-4f25-b103-a19761505a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445b88d4-df18-4f58-8b3a-71004161e1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61c00a5-af40-42d4-8e69-e141b7277b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
